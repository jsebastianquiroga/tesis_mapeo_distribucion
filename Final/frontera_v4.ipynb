{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Gd7nI8i2UC_"
      },
      "outputs": [],
      "source": [
        "#%run installs.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41_RmPng2UDB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from itertools import combinations\n",
        "import re\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import plotly.graph_objects as go\n",
        "import plotly.figure_factory as ff\n",
        "import cvxpy as cvx\n",
        "from scipy.stats import bernoulli, zscore\n",
        "from cvxpy import GUROBI as solverGUROBI\n",
        "from gurobipy import *\n",
        "import umap\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn_extra.cluster import KMedoids\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "class Frontera:\n",
        "    \"\"\"\n",
        "    The Frontera class represents a frontier in a high-dimensional space and is used for feature extraction and dimensionality reduction. \n",
        "    It takes as input the data points (X), their corresponding labels (y), and parameters for defining the frontier \n",
        "    (percentil_min, percentil_max, and N_points_frontera).\n",
        "    This class provides methods for calculating distances between points, as well as storing and managing information about categories, minimum distances, frontier points, and vectors.\n",
        "\n",
        "    There are two methods available for dimensionality reduction: 'frontier_reduction' and 'prototypes'.\n",
        "    'frontier_reduction': Finds the closest points that are aligned to the same direction vectors.\n",
        "    'prototypes': Finds prototypes on the frontier that have the majority of the groups.\n",
        "\n",
        "    Attributes:\n",
        "    - X: Data points\n",
        "    - y: Labels for data points\n",
        "    - percentil_min: Minimum percentile for the frontier\n",
        "    - percentil_max: Maximum percentile for the frontier\n",
        "    - N_points_frontera: Number of points for the frontier\n",
        "    - method: The dimensionality reduction method to use ('frontier_reduction' or 'prototypes')\n",
        "    - dic_categorias: Dictionary for storing category information\n",
        "    - dic_min_dst: Dictionary for storing minimum distances\n",
        "    - list_dist_median: List for storing median distances\n",
        "    - dic_categorias_UMAP: Dictionary for storing UMAP category information\n",
        "    - dic_min_dst_UMAP: Dictionary for storing UMAP minimum distances\n",
        "    - Frontier_Point_A_X: Dictionary for storing frontier points and their corresponding categories (A -> X)\n",
        "    - Frontier_Point_X_A: Dictionary for storing frontier points and their corresponding categories (X -> A)\n",
        "    - Frontier_Point: Dictionary for storing frontier points\n",
        "    - Frontier_Vector: Dictionary for storing frontier vectors\n",
        "    - class_vector: Dictionary for storing class vectors\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, method='frontier_reduction'):\n",
        "        self.X = X  # Data points\n",
        "        self.y = y  # Labels for data points\n",
        "\n",
        "        # Hard-code values for percentiles and number of points for the frontier\n",
        "        self.percentil_min = max(20, 0)  # Minimum percentile for the frontier\n",
        "        self.percentil_max = 20  # Maximum percentile for the frontier\n",
        "        self.N_points_frontera = int(0.20 * len(X))  # Number of points for the frontier\n",
        "\n",
        "        self.method = method \n",
        "        self.method = method  # Dimensionality reduction method ('frontier_reduction' or 'prototypes')\n",
        "        # Dictionaries for storing category information and minimum distances\n",
        "        self.dic_categorias = {}\n",
        "        self.dic_min_dst = {}\n",
        "        self.list_dist_median = []\n",
        "        \n",
        "        # Dictionaries for storing UMAP category information and minimum distances\n",
        "        self.dic_categorias_UMAP = {}\n",
        "        self.dic_min_dst_UMAP = {}\n",
        "        \n",
        "        # Dictionaries for storing frontier points and their corresponding categories\n",
        "        self.Frontier_Point_A_X = {}\n",
        "        self.Frontier_Point_X_A = {}\n",
        "        \n",
        "        # Dictionaries for storing frontier points, frontier vectors, and class vectors\n",
        "        self.Frontier_Point = {}\n",
        "        self.Frontier_Vector = {}\n",
        "        self.class_vector = {}\n",
        "\n",
        "        self._lambda = None\n",
        "        self.c_w_ = None\n",
        "        self.w_ = None\n",
        "        self.X_1 = None\n",
        "        self.y_1 = None\n",
        "        self.lengthX = None\n",
        "        self._epsilon = None\n",
        "        self.lengthL = None\n",
        "        self.label = None\n",
        "        self.data_info = None\n",
        "        # Variables for solving relaxed Integer Program.\n",
        "        self.xi_n_lin = None\n",
        "        self.alpha_j_lin = None\n",
        "        self.opt_val = None\n",
        "        # Variables for solving the Randomized Rounding Algorithm.\n",
        "        self.prototype_length = None\n",
        "        self.random_round_Sn = None\n",
        "        self.random_round_Aj = None\n",
        "        self.random_round_optimal_val = None\n",
        "        self.Protypes = {}\n",
        "        \n",
        "        self.num_vectors = None\n",
        "        self.random_vectors = None\n",
        "        self.groups = {}\n",
        "        self.centroids = {}\n",
        "        \n",
        "    def distance(self, x0, x1):\n",
        "        # This function calculates the pairwise squared Euclidean distance between two sets of points x0 and x1\n",
        "        # The squared Euclidean distance between two points x and y in Euclidean space is given by the formula:\n",
        "        # (x - y)^2 = x^2 - 2 * x * y + y^2\n",
        "        \n",
        "        # Calculate the pairwise squared Euclidean distance using the following steps:\n",
        "        # 1. Compute the squared norms of x0 and x1: x0^2 and x1^2\n",
        "        # 2. Compute the product of x0 and x1: x0 * x1\n",
        "        # 3. Compute the pairwise distance matrix M_distance using the formula: x0^2 - 2 * x0 * x1 + x1^2\n",
        "        \n",
        "        M_distance = np.reshape(\n",
        "            np.diag(np.dot(x0, x0.T)), (-1, 1)) - 2 * np.dot(x0, x1.T) + np.dot(np.ones((x0.shape[0], 1)), np.reshape(np.diag(np.dot(x1, x1.T)).T, (1, -1)))\n",
        "        return M_distance\n",
        "    def get_frontier(self):\n",
        "        # Iterate through unique elements in self.y\n",
        "        for i in np.unique(self.y): \n",
        "            # Create a dictionary with key format X_i and values being the corresponding X values when self.y is equal to i\n",
        "            select_ind = np.where(self.y == i)[0]\n",
        "            dic_categorias_aux = {'X_' + str(i): self.X[select_ind]}\n",
        "            # Update the main dictionary, self.dic_categorias\n",
        "            self.dic_categorias.update(dic_categorias_aux)\n",
        "\n",
        "        # Generate a list of all keys in self.dic_categorias\n",
        "        categorias_list = [key for key in self.dic_categorias]\n",
        "        # Generate all possible combinations of two categories\n",
        "        comb_categories = combinations(categorias_list, 2)\n",
        "        \n",
        "        # Iterate through all category combinations\n",
        "        for categories in list(comb_categories):\n",
        "            # Calculate distance between the two categories using self.distance\n",
        "            dist = self.distance(self.dic_categorias.get(categories[0]), self.dic_categorias.get(categories[1]))\n",
        "            # Append median of the distance to self.list_dist_median\n",
        "            self.list_dist_median.append(np.median(dist))\n",
        "\n",
        "            # Calculate row-wise mean of the distance matrix\n",
        "            row = np.mean(dist, axis=1)\n",
        "            # Select indices of rows that are within the specified percentiles\n",
        "            select_indices_row = np.where(\n",
        "                (row > np.percentile(row, self.percentil_min)) & (row < np.percentile(row, self.percentil_max)))[0]\n",
        "            # Get corresponding data points from the first category\n",
        "            min_dst_row = self.dic_categorias.get(categories[0])[select_indices_row]\n",
        "            # Update the self.dic_min_dst dictionary with the new values\n",
        "            dic_min_dst_aux = {categories[0] + '_with_' + categories[1]: min_dst_row}\n",
        "            self.dic_min_dst.update(dic_min_dst_aux)\n",
        "            # Calculate column-wise mean of the distance matrix\n",
        "            column = np.mean(dist, axis=0)\n",
        "            # Select indices of columns that are within the specified percentiles\n",
        "            select_indices_column = np.where(\n",
        "                (column > np.percentile(column, self.percentil_min)) & (column < np.percentile(column, self.percentil_max)))[0]\n",
        "            # Get corresponding data points from the second category\n",
        "            min_dst_column = self.dic_categorias.get(categories[1])[select_indices_column]\n",
        "            # Update the self.dic_min_dst dictionary with the new values\n",
        "            dic_min_dst_aux = {categories[1] + '_with_' + categories[0]: min_dst_column}\n",
        "            self.dic_min_dst.update(dic_min_dst_aux)\n",
        "            \n",
        "            #print(dist)\n",
        "            #break\n",
        "\n",
        "        # Generate a list of all keys in self.dic_min_dst\n",
        "        list_all_frontier = [key for key in self.dic_min_dst]\n",
        "        \n",
        "        # Iterate through unique elements in self.y\n",
        "        for i in np.unique(self.y):\n",
        "            # Compile regex patterns for matching dictionary keys\n",
        "            my_regex = r\"^X_\" + str(i)  # Regex pattern to match keys starting with \"X_\" and current element value\n",
        "            p = re.compile(my_regex)   # Compile regex pattern\n",
        "            list_A_with_X = [s for s in list_all_frontier if p.match(s)]  # Extract matching elements from list_all_frontier\n",
        "            \n",
        "            my_regex = r\".*\" + str(i) + r'$'  # Regex pattern to match keys ending with current element value\n",
        "            p = re.compile(my_regex)   # Compile regex pattern\n",
        "            list_X_with_A = [s for s in list_all_frontier if p.match(s)]  # Extract matching elements from list_all_frontier\n",
        "\n",
        "            # Get the first elements in the lists\n",
        "            mtz_A = self.dic_min_dst.get(list_A_with_X[0])  # Retrieve value from dic_min_dst using the first element in list_A_with_X\n",
        "            mtz_X = self.dic_min_dst.get(list_X_with_A[0])  # Retrieve value from dic_min_dst using the first element in list_X_with_A\n",
        "            \n",
        "            # Concatenate remaining elements to mtz_A and mtz_X\n",
        "            for j in range(1,len(list_A_with_X)):\n",
        "                mtz_A = np.concatenate((mtz_A,self.dic_min_dst.get(list_A_with_X[j])), axis=0)\n",
        "                mtz_X = np.concatenate((mtz_X,self.dic_min_dst.get(list_X_with_A[j])), axis=0)\n",
        "                        \n",
        "            # Create dictionaries to store updated values of mtz_A and mtz_X\n",
        "            Front_Point_A_X ={'FrontPoints:(' + str(i) + ',X)' : mtz_A}    \n",
        "            self.Frontier_Point_A_X.update(Front_Point_A_X)  # Update dictionary Frontier_Point_A_X with new values\n",
        "            Front_Point_X_A ={'FrontPoints:(X,' + str(i) + ')' : mtz_X}    \n",
        "            self.Frontier_Point_X_A.update(Front_Point_X_A)  # Update dictionary Frontier_Point_X_A with new values\n",
        "            \n",
        "    def centroid_regions(self):\n",
        "         ## CALCULAR LOS CENTROIDES EN REGION DE FRONTERA\n",
        "         # Compute the standard deviations for each class\n",
        "         std_devs = []\n",
        "         for label in np.unique(self.y):\n",
        "           std_devs.append(np.std(self.X[self.y == label], axis=0))\n",
        "           # Calculate the median of the standard deviations\n",
        "           median_std_dev = np.median(std_devs) * 1\n",
        "           # Loop through the dictionaries containing the frontier points\n",
        "           for (key_A,value_A), (key_X,value_X) in zip(self.Frontier_Point_A_X.items(), self.Frontier_Point_X_A.items()):\n",
        "            \n",
        "              # Make copies of the dictionaries to avoid modifying the original dictionaries\n",
        "              Point_A_X_copy = self.Frontier_Point_A_X.copy()\n",
        "              Point_X_A_copy = self.Frontier_Point_X_A.copy()\n",
        "              Front_Point = {}\n",
        "              \n",
        "              # Create a matrix to store the points and their coordinates\n",
        "              points_matriz = np.zeros(shape=(self.N_points_frontera, self.X.shape[1]))\n",
        "              \n",
        "              # Iterate through the points on the frontier\n",
        "              for i in range(0,self.N_points_frontera):\n",
        "                  \n",
        "                  # Calculate the distances between the A and X points\n",
        "                  dist = self.distance(Point_A_X_copy.get(key_A), Point_X_A_copy.get(key_X))\n",
        "                  \n",
        "                  # Find the indices of the minimum distance for row and column\n",
        "                  min_dist_A_with_X = np.where( dist==np.min(dist) )[0]\n",
        "                  min_dist_X_with_A = np.where( dist==np.min(dist) )[1]\n",
        "                  \n",
        "                  # Retrieve the A and X points with the minimum distance\n",
        "                  min_A_with_X = Point_A_X_copy.get(key_A)[min_dist_A_with_X]\n",
        "                  min_X_with_A = Point_X_A_copy.get(key_X)[min_dist_X_with_A]\n",
        "                  \n",
        "                  # Remove the selected points from the dictionaries\n",
        "                  Point_A_X_copy.update({ key_A: np.delete(Point_A_X_copy.get(key_A), min_dist_A_with_X, axis=0)})\n",
        "                  Point_X_A_copy.update({ key_X: np.delete(Point_X_A_copy.get(key_X), min_dist_X_with_A, axis=0)})\n",
        "                  \n",
        "                  # Calculate the centroid of the selected points and store it in the matrix\n",
        "                  point_value = (np.mean(min_A_with_X+min_X_with_A,axis=0))/2\n",
        "                  \n",
        "                  # Check for closeness with existing points and store only unique points\n",
        "                  if i != 0:\n",
        "                      closeness_criterion = self.distance(np.reshape(point_value, (-1, self.X.shape[1])), points_matriz) < median_std_dev\n",
        "\n",
        "                      if not np.any(closeness_criterion == True):\n",
        "                          points_matriz[i] = point_value\n",
        "                  else:\n",
        "                      points_matriz[i] = point_value\n",
        "                  # If one of the dictionaries is empty, exit the loop\n",
        "                  if (Point_A_X_copy.get(key_A).shape[0] == 0) or (Point_X_A_copy.get(key_X).shape[0] == 0):\n",
        "                      break\n",
        "              \n",
        "              # Remove unused rows from the matrix\n",
        "              row_points_matriz = np.sum(points_matriz, axis=1)\n",
        "              select_indices_points_matriz = np.where( row_points_matriz != 0 )[0]\n",
        "              points_matriz = points_matriz[select_indices_points_matriz]\n",
        "              \n",
        "              # Create a dictionary to store the frontier points\n",
        "              Front_Point ={'Frontier:' + key_A.split(':')[1] : points_matriz}    \n",
        "              self.Frontier_Point.update(Front_Point)  # Update the dictionary with the frontier points\n",
        "        \n",
        "    def calculate_vectors(self):\n",
        "        # Compute the standard deviations for each class\n",
        "        std_devs = []\n",
        "        for label in np.unique(self.y):\n",
        "            std_devs.append(np.std(self.X[self.y == label], axis=0))\n",
        "\n",
        "        # Calculate the median of the standard deviations\n",
        "        median_std_dev = np.median(std_devs)\n",
        "        # Loop through the categories and their associated frontier points\n",
        "        for (key_region,region), (key_Frontier,Frontier_Point) in zip(self.dic_categorias.items(), self.Frontier_Point.items()):\n",
        "            \n",
        "            # Create a matrix to store the vectors\n",
        "            Front_vector = {}\n",
        "            n_row = 0\n",
        "            vectors_matriz = np.zeros(shape=(self.Frontier_Point.get(key_Frontier).shape[0], self.X.shape[1]))\n",
        "            \n",
        "            # Iterate through the frontier points\n",
        "            for key_Frontier in self.Frontier_Point.get(key_Frontier):\n",
        "                # Calculate the distances between the category and the frontier point\n",
        "                dist = self.distance(self.dic_categorias.get(key_region), np.reshape(key_Frontier, (-1, self.X.shape[1])))\n",
        "\n",
        "                # Find the indices of the minimum distance\n",
        "                min_dist_A_with_B = np.where(dist <= median_std_dev)[0] #*********************\n",
        "                \n",
        "                # Retrieve the category points with the minimum distance\n",
        "                min_A_with_B = self.dic_categorias.get(key_region)[min_dist_A_with_B]        \n",
        "                \n",
        "                # Calculate the vector and store it in the matrix\n",
        "                vector_value = np.median(min_A_with_B,axis=0)\n",
        "                vectors_matriz[n_row] = vector_value\n",
        "            \n",
        "                n_row += 1\n",
        "            \n",
        "            # Create a dictionary to store the vectors for the current frontier\n",
        "            Front_vector ={'Vec_Frontier:(X,' + key_region.split('_')[-1] + ')' : vectors_matriz}    \n",
        "            self.Frontier_Vector.update(Front_vector) \n",
        "            \n",
        "        # Loop through the frontier points and their associated vectors\n",
        "        for (key_origin,value_origin), (key_vec,value_vector) in zip(self.Frontier_Point.items(), self.Frontier_Vector.items()):\n",
        "            \n",
        "            # Calculate the class vector by subtracting the frontier point from the vector\n",
        "            vector_aux = np.concatenate((value_origin,value_vector-value_origin), axis=1)\n",
        "            class_vec = {'Class_vector:' + key_vec.split(':')[1] : vector_aux}\n",
        "            self.class_vector.update(class_vec)\n",
        "    def filter_vector(self):\n",
        "        # Compute the standard deviations for each class\n",
        "        std_devs = []\n",
        "        for label in np.unique(self.y):\n",
        "            std_devs.append(np.std(self.X[self.y == label], axis=0))\n",
        "\n",
        "        # Calculate the median of the standard deviations\n",
        "        median_std_dev = np.median(std_devs)\n",
        "        # Combine all frontier points into a single matrix\n",
        "        mtz_origin = self.Frontier_Point.get([key for key in self.Frontier_Point][0])\n",
        "        for j in range(1, len(self.Frontier_Point)):\n",
        "            mtz_origin = np.concatenate((mtz_origin, self.Frontier_Point.get([key for key in self.Frontier_Point][j])), axis=0)\n",
        "\n",
        "        # Calculate distance matrix between all frontier points and replace diagonal with max distance\n",
        "        dist_mtz_origin = self.distance(mtz_origin, mtz_origin)\n",
        "        dis_max = np.max(dist_mtz_origin)\n",
        "        indent_max = np.nan_to_num(np.identity(dist_mtz_origin.shape[0]) * dis_max)\n",
        "        dist_mtz_origin = (dist_mtz_origin + indent_max)\n",
        "\n",
        "        # In the loop, replace the fixed value 2 with the median_std_dev\n",
        "        # Create a boolean matrix for distances less than median_std_dev\n",
        "        mtz_bool_eucl = dist_mtz_origin < median_std_dev\n",
        "\n",
        "\n",
        "        # Combine all class vectors into a single matrix\n",
        "        mtz_vec = self.class_vector.get([key for key in self.class_vector][0])\n",
        "        for j in range(1, len(self.class_vector)):\n",
        "            mtz_vec = np.concatenate((mtz_vec, self.class_vector.get([key for key in self.class_vector][j])), axis=0)\n",
        "        mtz_vec = mtz_vec[:, self.X.shape[1]:]\n",
        "\n",
        "        # First filter: filter by cosine distance\n",
        "        dist_cosine = 1 - pairwise_distances(mtz_vec, metric=\"cosine\")\n",
        "        mtz_bool_cos = dist_cosine <= -0.9\n",
        "\n",
        "        # Create an upper triangular boolean matrix\n",
        "        diag = np.triu(np.ones(dist_mtz_origin.shape), 1).T == 1\n",
        "\n",
        "        # Filter points based on boolean matrices and the upper triangular matrix\n",
        "        regla = mtz_bool_cos & mtz_bool_eucl & diag\n",
        "        select_indices_1 = np.where(regla == True)[0]\n",
        "\n",
        "        # Update origin and vectors based on first filter\n",
        "        origin_redu = mtz_origin[select_indices_1]\n",
        "        vec_redu = mtz_vec[select_indices_1]\n",
        "\n",
        "        # Store the corresponding labels for the filtered points\n",
        "        labels_reduct = self.y[select_indices_1]\n",
        "\n",
        "        # Update dictionaries with reduced frontier points and vectors\n",
        "        self.Frontier_Point.update({'Frontier: Reduct': origin_redu})\n",
        "        self.Frontier_Vector.update({'Vec_Frontier: Reduct': vec_redu + origin_redu})\n",
        "        self.class_vector.update({'Class_vector_Reduct:': np.concatenate((origin_redu, vec_redu), axis=1)})\n",
        "        \n",
        "        \n",
        "        \"\"\"\n",
        "        # Second filter: filter by cosine distance again\n",
        "        dist_cosine_2 = 1 - pairwise_distances(vec_redu, metric=\"cosine\")\n",
        "        mtz_bool_cos_2 = dist_cosine_2 <= 0.8\n",
        "\n",
        "        # Create an upper triangular boolean matrix\n",
        "        diag_2 = np.triu(np.ones(dist_cosine_2.shape), 1).T == 1\n",
        "\n",
        "        # Filter points based on boolean matrices and the upper triangular matrix\n",
        "        regla_2 = mtz_bool_cos_2 & diag_2\n",
        "        select_indices_1_1 = np.where(regla_2 == True)[0]\n",
        "\n",
        "        # Update origin and vectors based on second filter\n",
        "        origin_redu_2 = origin_redu[select_indices_1_1]\n",
        "        vec_redu_2 = vec_redu[select_indices_1_1]\n",
        "\n",
        "        # Update dictionaries with fully reduced frontier points and vectors\n",
        "        Front_Point = {'Frontier: Full Reduct': origin_redu_2}\n",
        "        self.Frontier_Point.update(Front_Point)\n",
        "        Front_vector ={'Vec_Frontier: Full Reduct' : vec_redu_2+origin_redu_2}    \n",
        "        self.Frontier_Vector.update(Front_vector) \n",
        "        class_vec = {'Class_vector_Full_Reduct:': np.concatenate((origin_redu_2,vec_redu_2), axis=1)}\n",
        "        self.class_vector.update(class_vec) \n",
        "        \"\"\"\n",
        "        \n",
        "        self.color_list = [0] * len(self.dic_categorias.keys())\n",
        "        for i in range( len(self.dic_categorias.keys()) ):\n",
        "            self.color_list[i] = np.random.randint(0, 1000)\n",
        "        # Return the vectors, their dimensions, and the labels\n",
        "        return vec_redu, labels_reduct #, vec_redu.shape\n",
        "#    --------------------------------------------------------------------\n",
        "\n",
        "    def get_X1_Y1(self):\n",
        "        \"\"\"\n",
        "        Extracts X_1 and y_1 arrays based on the Frontier_Point_A_X attribute.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        X_1 : numpy array\n",
        "            Array containing the data points corresponding to the frontier points.\n",
        "        y_1 : numpy array\n",
        "            Array containing the labels corresponding to the frontier points.\n",
        "        \"\"\"\n",
        "        X_1 = []\n",
        "        y_1 = []\n",
        "        for key, value in self.Frontier_Point_A_X.items():\n",
        "            for point in value:\n",
        "                X_1.append(point)\n",
        "                #y_1.append(int(''.join(filter(str.isdigit, key.split(':')[1]))))\n",
        "                y_1.append(0)\n",
        "        self.X_1 = np.array(X_1)\n",
        "        self.y_1 = np.array(y_1)\n",
        "        print(\"Shape of self.X_1:\", self.X_1.shape)\n",
        "        print(\"Content of self.X_1:\", self.X_1)\n",
        "        if len(self.X_1.shape) > 1:\n",
        "            self._epsilon = np.percentile(self.X_1[:, 1], 5)\n",
        "        else:\n",
        "            self._epsilon = np.percentile(self.X_1, 5)\n",
        "        #X_1[:,1].mean() #assigne the mean of the distances to _epsilon\n",
        "\n",
        "\n",
        "    def labelsInfo(self):\n",
        "        \"\"\" Extracts the unique class label from the data X.\n",
        "\n",
        "            Attributes\n",
        "            ----------\n",
        "            self.label : set \n",
        "                Stores unique class labels.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            set of class labels.\n",
        "        \"\"\"\n",
        "        self.label = set(self.y_1)\n",
        "        return self.label\n",
        "\n",
        "    def getNumLabel(self):\n",
        "        \"\"\" Extracts the length of the class labels.\n",
        "\n",
        "            Attributes\n",
        "            ----------\n",
        "            self.lengthL : int\n",
        "                length of the class labels.\n",
        "            \n",
        "            Returns\n",
        "            -------\n",
        "            self.length : int\n",
        "                length of the class labels.\n",
        "           \n",
        "        \"\"\"\n",
        "        self.lengthL = len(self.label)\n",
        "        return self.lengthL \n",
        "\n",
        "    def dataInfo(self, label):\n",
        "        \"\"\"Creates a data dictionary according to the class label passed as argument label.\n",
        "\n",
        "            Parameter\n",
        "            ---------\n",
        "            label: int\n",
        "                contains the label number.\n",
        "\n",
        "            Attributes\n",
        "            ----------\n",
        "            indexes: list[int]\n",
        "                indices, where label is equal to the class label of the data.\n",
        "            X_l: data\n",
        "                subset of the training data with indexes as index.\n",
        "            y_l: data\n",
        "                class label of the data X_l.\n",
        "            size_l: int\n",
        "                length of the data with the same label.\n",
        "            indexes_not_l: list[int]\n",
        "                indices, where label is not equal to the class label of the data.\n",
        "            X_not_l: data\n",
        "                subset of data where the label is not equal to the class label.\n",
        "            y_not_l: data\n",
        "                contains the label which is not equal to the label.\n",
        "            size_not_l: list[int]\n",
        "                length of the data which is not of the same label.\n",
        "            data: dict\n",
        "                contains the data where argument label is equal to the class label.\n",
        "            \n",
        "            Returns\n",
        "            -------\n",
        "            data: data dictionary containing data with class label equal to label and class label not\n",
        "                equal to label.\n",
        "        \"\"\"\n",
        "        indexes = [idx for idx, lbl in enumerate(self.y_1) if lbl == label]\n",
        "        X_l = self.X_1[indexes]\n",
        "        y_l = self.y_1[indexes]\n",
        "        size_l = X_l.shape[0]\n",
        "        indexes_not_l = [idx for idx, lbl in enumerate(self.y_1) if lbl != label]\n",
        "        X_not_l = self.X_1[indexes_not_l]\n",
        "        y_not_l = self.y_1[indexes_not_l]\n",
        "        size_not_l = X_not_l.shape[0]\n",
        "        data = {\n",
        "            \"X_l\": X_l,\n",
        "            \"y_l\": y_l,\n",
        "            \"indices_l\": indexes,\n",
        "            \"size_l\": size_l,\n",
        "            \"X_not_l\": X_not_l,\n",
        "            \"y_not_l\": y_not_l,\n",
        "            \"indices_not_l\": indexes_not_l,\n",
        "            \"size_not_l\": size_not_l,\n",
        "        }\n",
        "        return data\n",
        "\n",
        "    def checkNeighborhood(self, x_1, x_2):\n",
        "        \"\"\" Checks if a point is in the epsilon ball or if a point 'x_test' is in the epsilon neighborhood of a\n",
        "            point 'x'.\n",
        "        \n",
        "            Parameter\n",
        "            ---------\n",
        "            x_1: vector\n",
        "                center of the epsilon ball.\n",
        "            x_2: vector\n",
        "                point to be checked if it lies in the ball with radius epsilon or not.\n",
        "            \n",
        "            Attribute\n",
        "            ---------\n",
        "                result: bool\n",
        "                    Checks if the point is in neighbourhood or not.\n",
        "            \n",
        "            Returns\n",
        "            -------\n",
        "                True, if a point lies in the neighborhood of epsilon ball centered around parameter\n",
        "                x otherwise, returns False.\n",
        "        \"\"\"\n",
        "        result = np.linalg.norm((x_1 - x_2), ord=2, keepdims=True) <= self._epsilon\n",
        "        return result\n",
        "\n",
        "    def calculate_Clj(self, label):\n",
        "        \"\"\" Calculates the total number of data samples(with different label) if it comes in the epsilon \n",
        "            neighborhood of a point if a point is chosen as a prototype.\n",
        "\n",
        "            Parameter \n",
        "            ---------\n",
        "            label: int\n",
        "                contains a class label.\n",
        "            \n",
        "            Attributes\n",
        "            ----------\n",
        "            data: dict\n",
        "                contains the data dictionary according to the argument label.\n",
        "            X_l: Any\n",
        "                contains the data with same class label.\n",
        "            X_not_l: Any\n",
        "                contains the data where label is not equal to class label.\n",
        "            sets_C_lj: list[int]\n",
        "                contains number of points covered where, class label is not equal to true label if x_jl is\n",
        "                considered as prototype.\n",
        "            \n",
        "            Returns\n",
        "            -------\n",
        "            Cost of adding a point.\n",
        "        \"\"\"\n",
        "        data = self.dataInfo(label=label)\n",
        "        X_l = data[\"X_l\"]\n",
        "        X_not_l = data[\"X_not_l\"]\n",
        "        sets_C_lj = []\n",
        "        for x_jl in X_l:\n",
        "            temp = 0\n",
        "            for x_no_l in X_not_l:\n",
        "                if self.checkNeighborhood(x_jl, x_no_l):\n",
        "                    temp += 1\n",
        "            sets_C_lj.append(temp)\n",
        "        C_lj = [(self._lambda + set_C_lj) for set_C_lj in sets_C_lj]\n",
        "        data[\"C_lj\"] = np.array(C_lj)\n",
        "        return data\n",
        "\n",
        "    def checkCoverPoints(self, label):\n",
        "        \"\"\" Checks for neighborhood points, if a point is in the ball with radius epsilon(for the\n",
        "            chosen data as a prototype) then, the value for that index is 1 else it is zero.\n",
        "\n",
        "            Parameter\n",
        "            ---------\n",
        "            label: int\n",
        "                Contains a class label.\n",
        "\n",
        "            Attributes\n",
        "            ----------\n",
        "            X_l: data\n",
        "                subset of the training data with indexes as index.\n",
        "            size_l: int\n",
        "                length of the data with the same label.\n",
        "            constraint_matrix: int\n",
        "                size_l X size_l matrix which contains the neighborhood information.\n",
        "\n",
        "            Returns\n",
        "            -------\n",
        "            data:  updates the data dictionary with pairwise neighborhood information.\n",
        "        \"\"\"\n",
        "        data = self.calculate_Clj(label=label)\n",
        "        X_l = data[\"X_l\"]\n",
        "        size_l = data[\"size_l\"]\n",
        "        constraint_matrix = np.zeros(shape=(size_l, size_l))\n",
        "        for x_nl in range(size_l):\n",
        "            for x_jl in range(size_l):\n",
        "                if self.checkNeighborhood(X_l[x_jl], X_l[x_nl]):\n",
        "                    constraint_matrix[x_nl][x_jl] = 1\n",
        "        data[\"constraint_matrix\"] = constraint_matrix\n",
        "        return data\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\" Here the model is trained according to the class label using convex optimisation method.\n",
        "            Attributes are added to the data dictionary for Randomized rounding algorithm for optimal\n",
        "            parameters. \n",
        "\n",
        "            Attributes\n",
        "            ----------\n",
        "            train_X: M x N matrix\n",
        "                Training data set.\n",
        "            train_y: M x 1 vector\n",
        "                Class label of the training data set.\n",
        "            label: int\n",
        "                contains set of unique labels.\n",
        "            alpha_j_lin: vector\n",
        "                stores the alpha value after optimisation.\n",
        "            xi_n_lin: vector\n",
        "                stores Xi value after optimisation.\n",
        "            opt_val_lin: list\n",
        "                contains the optimal value of the Objective function.\n",
        "            update_data_info:\n",
        "                updates the data dictionary with the attributes above.\n",
        "        \"\"\"\n",
        "        self.lengthX = len(self.X_1)\n",
        "        self._lambda = 1/ self.lengthX\n",
        "        label = self.labelsInfo()\n",
        "        alpha_j_lin = np.zeros(shape=self.lengthX)\n",
        "        xi_n_lin = np.zeros(shape=self.lengthX)\n",
        "        opt_val_lin = []\n",
        "        update_data_info = []\n",
        "\n",
        "        solverGUROBI = cvx.GUROBI\n",
        "\n",
        "        for lbl in label:\n",
        "            data = self.checkCoverPoints(label=lbl)\n",
        "            size_l = data[\"size_l\"]\n",
        "            C_lj = data[\"C_lj\"]\n",
        "            alpha_jl = cvx.Variable(shape=size_l)\n",
        "            xi_nl = cvx.Variable(shape=size_l)\n",
        "            constraint_matrix = data[\"constraint_matrix\"]\n",
        "            zero_vec = np.zeros(shape=size_l)\n",
        "            one_vec = np.ones(shape=size_l)\n",
        "            # Objective function for minimisation.\n",
        "            objective = cvx.Minimize((C_lj @ alpha_jl) + sum(xi_nl))\n",
        "            # Constraints for the minimisation.\n",
        "            constraints = [(constraint_matrix @ alpha_jl >= (one_vec - xi_nl)),\n",
        "                           alpha_jl >= zero_vec,\n",
        "                           alpha_jl <= one_vec,\n",
        "                           xi_nl >= zero_vec]\n",
        "\n",
        "            prob = cvx.Problem(objective, constraints)\n",
        "            prob.solve(solver=solverGUROBI)\n",
        "\n",
        "            data[\"alpha_jl_lp\"] = [1 if alpha_jl >= 1 else alpha_jl for alpha_jl in alpha_jl.value]\n",
        "            data[\"alpha_jl_lp\"] = [0 if alpha_jl <= 0 else alpha_jl for alpha_jl in data[\"alpha_jl_lp\"]]\n",
        "            data[\"xi_nl_lp\"] = [1 if xi_nl >= 1 else xi_nl for xi_nl in xi_nl.value]\n",
        "            data[\"xi_nl_lp\"] = [0 if xi_nl <= 0 else xi_nl for xi_nl in data[\"xi_nl_lp\"]]\n",
        "\n",
        "            first_term = sum([(c_lj * alpha_jl) for c_lj, alpha_jl in zip(data[\"C_lj\"], data[\"alpha_jl_lp\"])])\n",
        "            second_term = sum(data[\"xi_nl_lp\"])\n",
        "            # calculate the optimal value after the optimisation.\n",
        "            optimal_l = first_term + second_term\n",
        "\n",
        "            data[\"optimal_l_lp\"] = optimal_l\n",
        "            opt_val_lin.append(optimal_l)\n",
        "            update_data_info.append(data)\n",
        "\n",
        "            indices_l = data[\"indices_l\"]\n",
        "            for idx in indices_l:\n",
        "                alpha_j_lin[idx] = alpha_jl.value[indices_l.index(idx)]\n",
        "                xi_n_lin[idx] = xi_nl.value[indices_l.index(idx)]\n",
        "\n",
        "        self.alpha_j_lin = alpha_j_lin\n",
        "        self.xi_n_lin = xi_n_lin\n",
        "        self.opt_val = sum(opt_val_lin)\n",
        "        self.data_info = update_data_info\n",
        "        self.objectiveValue()\n",
        "\n",
        "        return self.w_, self.c_w_\n",
        "\n",
        "    def objectiveValue(self):\n",
        "        \"\"\" Here Randomized Rounding Algorithm is performed to recover the integer values we need \n",
        "            for our indicator variable, we choose to round our optimal variables of the linear program.\n",
        "\n",
        "            Attributes\n",
        "            ----------\n",
        "            data_info:\n",
        "\n",
        "            label:\n",
        "                 contains set of unique labels.\n",
        "            random_round_Aj:\n",
        "\n",
        "            random_round_Sn:\n",
        "            random_round_optimal_value:\n",
        "            update_data_info:\n",
        "        \"\"\"\n",
        "        data_info = self.data_info\n",
        "        label = self.labelsInfo()\n",
        "        random_round_Aj = np.zeros(shape=self.lengthX)\n",
        "        random_round_Sn = np.zeros(shape=self.lengthX)\n",
        "        random_round_optimal_value = []\n",
        "        update_data_info = []\n",
        "        for lbl in label:\n",
        "            data = data_info[lbl]\n",
        "            size_l = data[\"size_l\"]\n",
        "            temp_random_round_Ajl = np.zeros(shape=size_l, dtype=int)\n",
        "            temp_random_round_Snl = np.zeros(shape=size_l, dtype=int)\n",
        "            temp_optimal_round = float('nan')\n",
        "            C_lj = data[\"C_lj\"]\n",
        "            alpha_jl_lp = data[\"alpha_jl_lp\"]\n",
        "            xi_nl_lp = data[\"xi_nl_lp\"]\n",
        "            optimal_l_lp = data[\"optimal_l_lp\"]\n",
        "            optimal_l_log = 2 * np.log (size_l) * optimal_l_lp\n",
        "            repeat = True\n",
        "            while repeat:\n",
        "                temp_random_round_Ajl = np.zeros(shape=size_l, dtype=int)\n",
        "                temp_random_round_Snl = np.zeros(shape=size_l, dtype=int)\n",
        "\n",
        "                for l in range(int(np.ceil(2 * np.log(size_l)))):\n",
        "                    for j in range(size_l):\n",
        "                        temp_round_Ajl = bernoulli.rvs(alpha_jl_lp[j])\n",
        "                        temp_random_round_Ajl[j] = max(temp_round_Ajl, temp_random_round_Ajl[j])\n",
        "                        temp_round_Snl = bernoulli.rvs(xi_nl_lp[j])\n",
        "                        temp_random_round_Snl[j] = max(temp_round_Snl, temp_random_round_Snl[j])\n",
        "\n",
        "                constraint_matrix = data[\"constraint_matrix\"]\n",
        "                first_term = sum([c_lj * A_jl for c_lj, A_jl in zip(C_lj, temp_random_round_Ajl)])\n",
        "                second_term = sum(temp_random_round_Snl)\n",
        "                temp_optimal_round = first_term + second_term\n",
        "\n",
        "                get_val_Ajl = constraint_matrix @ temp_random_round_Ajl\n",
        "                get_val_Sn = 1 - temp_random_round_Snl\n",
        "                if all ([lhs >= rhs for lhs, rhs in zip(get_val_Ajl, get_val_Sn)]):\n",
        "                    if all ([(A_jl == 0 or A_jl == 1) for A_jl in temp_random_round_Ajl]):\n",
        "                        if all ([S_nl >= 0 for S_nl in temp_random_round_Snl]):\n",
        "                            if temp_optimal_round <= optimal_l_log:\n",
        "                                repeat = False\n",
        "\n",
        "            data[\"random_round_Ajl\"] = temp_random_round_Ajl\n",
        "            data[\"random_round_Snl\"] = temp_random_round_Snl\n",
        "            data[\"optimal_val_random\"] = temp_optimal_round\n",
        "            random_round_optimal_value.append(temp_optimal_round)\n",
        "            update_data_info.append(data)\n",
        "\n",
        "            indices_l = data[\"indices_l\"]\n",
        "            for idx in indices_l:\n",
        "                random_round_Aj[idx] = temp_random_round_Ajl[indices_l.index(idx)]\n",
        "                random_round_Sn[idx] = temp_random_round_Snl[indices_l.index(idx)]\n",
        "\n",
        "        self.random_round_Aj = random_round_Aj\n",
        "        self.random_round_Sn = random_round_Sn\n",
        "        self.random_round_optimal_val = sum(random_round_optimal_value)\n",
        "\n",
        "        self.w_ = [x for x, A_j in zip(self.X_1, self.random_round_Aj) if A_j == 1]\n",
        "        self.w_ = np.array (self.w_)\n",
        "\n",
        "        self.c_w_ = [y for y, A_j in zip(self.y_1, self.random_round_Aj) if A_j == 1]\n",
        "        self.prototype_length = self.w_.shape[0]\n",
        "        self.data_info = update_data_info\n",
        "        \n",
        "        self.Protypes.update({'Protypes' : self.w_}) \n",
        "\n",
        "# prototypes = proto_selector.w_\n",
        "# prototype_labels = proto_selector.c_w_\n",
        "#-----------------------------------------------------------------------------------\n",
        "\n",
        "    def create_groups(self):\n",
        "        \"\"\"\n",
        "        Creates groups based on the dot product of the projection of input data\n",
        "        on the random vectors.\n",
        "        \"\"\"\n",
        "        \n",
        "        unique_vals = np.unique(self.y)\n",
        "        self.num_vectors = np.amax([self.X.shape[1],len(np.unique(self.y)-1)], axis=0)\n",
        "        \n",
        "        X_z_score = zscore(self.X_1)\n",
        "        \n",
        "        M_vectors = np.random.randn(self.X_1.shape[1], self.num_vectors)\n",
        "        R = X_z_score@M_vectors\n",
        "        S = np.where(R>0, 1, 0)\n",
        "        \n",
        "        # column vector to convert binary vector to integer e.g. (1,0,1)->5\n",
        "        binary_column = 2**np.arange(self.num_vectors).reshape(-1,  1)\n",
        "        \n",
        "        # convert each band into a single integer, \n",
        "        # i.e. convert band matrices to band columns\n",
        "        S = np.hstack([M@binary_column for M in S])\n",
        "                    \n",
        "        # Create a dictionary to store the LSH points\n",
        "        for group in np.unique(S):\n",
        "            select_indices_ = np.where( S == group )[0]\n",
        "            points_by_groups = self.X_1[select_indices_]\n",
        "            LSH_Point = {str(group) : points_by_groups}\n",
        "            self.groups.update(LSH_Point)\n",
        "            \n",
        "\n",
        "    def compute_centroids(self):\n",
        "        \"\"\"\n",
        "        Computes the centroid coordinates for each group by averaging the variables.\n",
        "        \"\"\"\n",
        "        if self.groups is None:\n",
        "            self.create_groups()\n",
        "\n",
        "        self.centroids = {}\n",
        "        self.centroid_indices = {}  # New dictionary to store the indices of the points in each group\n",
        "        for group_key, group_data in self.groups.items():\n",
        "            group_matrix = np.array(group_data)\n",
        "            centroid = np.mean(group_matrix, axis=0)\n",
        "            LSH_centroid = {'Centroid Group: ' + group_key : centroid}\n",
        "            self.centroids.update(LSH_centroid)\n",
        "\n",
        "            # Store the indices of the points in each group\n",
        "            select_indices_ = np.where(S == int(group_key))[0]\n",
        "            self.centroid_indices.update({group_key: select_indices_})\n",
        "\n",
        "    def fit_lsh(self):\n",
        "        \"\"\"\n",
        "        Fits the LSH model to the input data and computes the centroids.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            A dictionary containing the centroid coordinates for each group.\n",
        "        list\n",
        "            A list of y_label associated with the centroid of each group.\n",
        "        \"\"\"\n",
        "        self.compute_centroids()\n",
        "        centroid_arrays = list(self.centroids.values())\n",
        "\n",
        "        # Get the y_label associated with the centroid of each group\n",
        "        centroid_labels = []\n",
        "        for group_key, indices in self.centroid_indices.items():\n",
        "            y_labels = self.y[indices].tolist()\n",
        "            centroid_labels.append(y_labels)\n",
        "\n",
        "        # Compute the median y_label for each centroid\n",
        "        centroid_median_labels = [np.median(y_labels) for y_labels in centroid_labels]\n",
        "\n",
        "        return centroid_arrays, centroid_median_labels\n",
        "#    --------------------------------------------------------------------\n",
        "    def find_optimal_clusters(X, y, max_k=10):\n",
        "        inerties = []\n",
        "        silhouette_scores = []\n",
        "\n",
        "        for k in range(2, max_k+1):\n",
        "            kmedoids = KMedoids(n_clusters=k, random_state=42).fit(X)\n",
        "            inerties.append(kmedoids.inertia_)\n",
        "            silhouette_scores.append(silhouette_score(X, kmedoids.labels_))\n",
        "\n",
        "        # Find the optimal number of clusters (the elbow point)\n",
        "        optimal_k = np.argmin(np.diff(inerties)) + 3  # Add 3 because we started from k=2\n",
        "\n",
        "        # Train the KMedoids model with the optimal number of clusters\n",
        "        optimal_kmedoids = KMedoids(n_clusters=optimal_k, random_state=42).fit(X)\n",
        "\n",
        "        # Return the clustered points and their associated labels\n",
        "        clustered_points = {i: X[optimal_kmedoids.labels_ == i] for i in range(optimal_k)}\n",
        "        cluster_labels = {i: y[optimal_kmedoids.labels_ == i] for i in range(optimal_k)}\n",
        "\n",
        "        # Compute the median y_label for each cluster\n",
        "        cluster_median_y_labels = {i: np.median(cluster_labels[i]) for i in range(optimal_k)}\n",
        "\n",
        "        return clustered_points, cluster_median_y_labels # cluster_labels, \n",
        "#    --------------------------------------------------------------------        \n",
        "    def frontier(self, X=None, y=None):\n",
        "        if self.method == 'frontier_reduction':\n",
        "            self.get_frontier()\n",
        "            self.centroid_regions()\n",
        "            self.calculate_vectors()\n",
        "            self.filter_vector()\n",
        "            return vec_redu, labels_reduct\n",
        "\n",
        "        elif self.method == 'prototypes':\n",
        "            self.get_frontier()\n",
        "            self.get_X1_Y1()\n",
        "            self.fit()\n",
        "            return self.w_, self.c_w_\n",
        "\n",
        "        elif self.method == 'LSH':\n",
        "            self.get_frontier()\n",
        "            self.get_X1_Y1()\n",
        "            self.create_groups()\n",
        "            self.fit_lsh()\n",
        "            return centroid_arrays, centroid_median_labels\n",
        "\n",
        "        elif self.method == 'KM':\n",
        "            self.get_frontier()\n",
        "            self.get_X1_Y1()\n",
        "            clustered_points, cluster_median_y_labels = self.find_optimal_clusters(X, y)\n",
        "            return clustered_points, cluster_median_y_labels \n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid method specified\")\n",
        "\n",
        "\n",
        "#    --------------------------------------------------------------------\n",
        "    def plot_muestra_2D(self, col_1, col_2, include_layout=True):\n",
        "        # Set initial state for the loop\n",
        "        door = True\n",
        "        next_color = 0\n",
        "\n",
        "        # Loop through the categories in the dictionary\n",
        "        for key, value in self.dic_categorias.items():\n",
        "            # If it's the first iteration, create a new plot with the first category's data\n",
        "            if door:\n",
        "                fig = go.Figure(data=[go.Scatter(x=value[:, col_1], y=value[:, col_2],\n",
        "                                                 mode='markers',\n",
        "                                                 name=key,\n",
        "                                                 marker=dict(\n",
        "                                                     size=6,\n",
        "                                                     # Set color to an array/list of desired values\n",
        "                                                     # color=self.color_list[next_color],\n",
        "                                                     colorscale='picnic',  # Choose a colorscale\n",
        "                                                     opacity=0.7)\n",
        "                                                 )])\n",
        "                # Change the door variable to False, indicating that the first iteration is done\n",
        "                door = False\n",
        "                # Increment the color index\n",
        "                next_color += 1\n",
        "            else:\n",
        "                # For the rest of the categories, add them to the existing plot\n",
        "                fig.add_trace(go.Scatter(x=value[:, col_1], y=value[:, col_2],\n",
        "                                         mode='markers',\n",
        "                                         name=key,\n",
        "                                         marker=dict(\n",
        "                                             size=6,\n",
        "                                             # Set color to an array/list of desired values\n",
        "                                             # color=self.color_list[next_color],\n",
        "                                             colorscale='picnic',  # Choose a colorscale\n",
        "                                             opacity=0.7)\n",
        "                                         ))\n",
        "                # Increment the color index\n",
        "                next_color += 1\n",
        "\n",
        "        # If the include_layout parameter is True, update the layout properties of the plot\n",
        "        if include_layout:\n",
        "            fig.update_layout(\n",
        "                autosize=False,\n",
        "                width=600,\n",
        "                height=600,\n",
        "                margin=dict(l=0, r=0, b=0, t=10))\n",
        "\n",
        "            # Display the plot\n",
        "            fig.show()\n",
        "            \n",
        "    def plot_frontera_2D(self, col_1, col_2):\n",
        "        # Set initial state for the loop\n",
        "        door = True\n",
        "        next_color = 0\n",
        "\n",
        "        # Loop through the categories in the dictionary\n",
        "        for key, value in self.dic_categorias.items():\n",
        "            # If it's the first iteration, create a new plot with the first category's data\n",
        "            if door:\n",
        "                fig = go.Figure(data=[go.Scatter(x=value[:, col_1], y=value[:, col_2],\n",
        "                                                 mode='markers',\n",
        "                                                 name=key,\n",
        "                                                 marker=dict(\n",
        "                                                     size=6,\n",
        "                                                     # Set color to an array/list of desired values\n",
        "                                                     # color=self.color_list[next_color],\n",
        "                                                     colorscale='picnic',  # Choose a colorscale\n",
        "                                                     opacity=0.7)\n",
        "                                                 )])\n",
        "                # Change the door variable to False, indicating that the first iteration is done\n",
        "                door = False\n",
        "                # Increment the color index\n",
        "                next_color += 1\n",
        "            else:\n",
        "                # For the rest of the categories, add them to the existing plot\n",
        "                fig.add_trace(go.Scatter(x=value[:, col_1], y=value[:, col_2],\n",
        "                                         mode='markers',\n",
        "                                         name=key,\n",
        "                                         marker=dict(\n",
        "                                             size=6,\n",
        "                                             # Set color to an array/list of desired values\n",
        "                                             # color=self.color_list[next_color],\n",
        "                                             colorscale='picnic',  # Choose a colorscale\n",
        "                                             opacity=0.7)\n",
        "                                         ))\n",
        "                # Increment the color index\n",
        "                next_color += 1\n",
        "\n",
        "        # Loop through the dictionaries containing frontier points and add them to the plot\n",
        "        for point_dict in [self.Frontier_Point_A_X, self.Frontier_Point_X_A, self.Frontier_Point]:\n",
        "            for key, value_dst in point_dict.items():\n",
        "                # Configure the marker symbol and size based on the dictionary being processed\n",
        "                if point_dict == self.Frontier_Point:\n",
        "                    symbol = 300\n",
        "                    size = 50\n",
        "                else:\n",
        "                    symbol = 220\n",
        "                    size = 14\n",
        "\n",
        "                # Add the frontier points to the plot\n",
        "                fig.add_trace(go.Scatter(x=value_dst[:, col_1], y=value_dst[:, col_2],\n",
        "                                         mode='markers',\n",
        "                                         name=key,\n",
        "                                         marker=dict(\n",
        "                                             symbol=symbol,\n",
        "                                             size=size,\n",
        "                                             color=np.random.randint(100),  # Set color to a random integer value\n",
        "                                             # colorscale='Viridis',  # Choose a colorscale\n",
        "                                             opacity=1)\n",
        "                                         ))\n",
        "                \n",
        "        for key, value in self.Protypes.items():\n",
        "            fig.add_trace(go.Scatter(x=value[:, col_1], y=value[:, col_2],\n",
        "                                     mode='markers',\n",
        "                                     name=key,\n",
        "                                     marker=dict(\n",
        "                                         symbol=300,\n",
        "                                         size=50,\n",
        "                                         color=np.random.randint(100),  # Set color to a random integer value\n",
        "                                         # colorscale='Viridis',  # Choose a colorscale\n",
        "                                         opacity=1)\n",
        "                                    ))\n",
        "            \n",
        "        for key, value in self.centroids.items():\n",
        "            fig.add_trace(go.Scatter(x=[value[0]], y=[value[1]],\n",
        "                                     mode='markers',\n",
        "                                     name=key,\n",
        "                                     marker=dict(\n",
        "                                         symbol=300,\n",
        "                                         size=50,\n",
        "                                         color=np.random.randint(100),  # Set color to a random integer value\n",
        "                                         # colorscale='Viridis',  # Choose a colorscale\n",
        "                                         opacity=1)\n",
        "                                    )) \n",
        "\n",
        "        # Update the layout properties of the plot\n",
        "        fig.update_layout(\n",
        "            autosize=True,\n",
        "            width=1200,\n",
        "            height=750,\n",
        "            margin=dict(l=10, r=10, b=10, t=20))\n",
        "\n",
        "        # Display the plot\n",
        "        fig.show()\n",
        "       \n",
        "        \n",
        "        \n",
        "    def plot_Vectors(self, col_1, col_2):\n",
        "        # Set initial state for the loop\n",
        "        door = True\n",
        "        next_color = 0\n",
        "\n",
        "        # Loop through the categories in the dictionary\n",
        "        for key, value in self.dic_categorias.items():\n",
        "            # If it's the first iteration, create a new plot with the first category's data\n",
        "            if door:\n",
        "                fig = go.Figure(data=[go.Scatter(x=value[:, col_1], y=value[:, col_2],\n",
        "                                                 mode='markers',\n",
        "                                                 name=key,\n",
        "                                                 marker=dict(\n",
        "                                                     size=6,\n",
        "                                                     # Set color to an array/list of desired values\n",
        "                                                     # color=self.color_list[next_color],\n",
        "                                                     colorscale='picnic',  # Choose a colorscale\n",
        "                                                     opacity=0.7)\n",
        "                                                 )])\n",
        "                # Change the door variable to False, indicating that the first iteration is done\n",
        "                door = False\n",
        "                # Increment the color index\n",
        "                next_color += 1\n",
        "            else:\n",
        "                # For the rest of the categories, add them to the existing plot\n",
        "                fig.add_trace(go.Scatter(x=value[:, col_1], y=value[:, col_2],\n",
        "                                         mode='markers',\n",
        "                                         name=key,\n",
        "                                         marker=dict(\n",
        "                                             size=6,\n",
        "                                             # Set color to an array/list of desired values\n",
        "                                             # color=self.color_list[next_color],\n",
        "                                             colorscale='picnic',  # Choose a colorscale\n",
        "                                             opacity=0.7)\n",
        "                                         ))\n",
        "                # Increment the color index\n",
        "                next_color += 1\n",
        "\n",
        "        # Loop through the dictionaries containing frontier points and vectors\n",
        "        for point_dict, symbol, size in zip([self.Frontier_Point, self.Frontier_Vector], [300, 2], [50, 15]):\n",
        "            for key, value_dst in point_dict.items():\n",
        "                # Add the points or vectors to the plot\n",
        "                fig.add_trace(go.Scatter(x=value_dst[:, col_1], y=value_dst[:, col_2],\n",
        "                                         mode='markers',\n",
        "                                         name=key,\n",
        "                                         marker=dict(\n",
        "                                             symbol=symbol,\n",
        "                                             size=size,\n",
        "                                             color=np.random.randint(100),  # Set color to a random integer value\n",
        "                                             # colorscale='Viridis',  # Choose a colorscale\n",
        "                                             opacity=1)\n",
        "                                         ))\n",
        "\n",
        "        # Loop through the class vectors and create quiver plots\n",
        "        for key, value in self.class_vector.items():\n",
        "            # Calculate the midpoints for the u and v components of the quiver plot\n",
        "            col_1_ = int(col_1 + value.shape[1] / 2)\n",
        "            col_2_ = int(col_2 + value.shape[1] / 2)\n",
        "\n",
        "            # Create a quiver plot for the class vector\n",
        "            quiver_fig = ff.create_quiver(x=value[:, col_1], y=value[:, col_2],\n",
        "                                          u=value[:, col_1_], v=value[:, col_2_],\n",
        "                                          scale=1,\n",
        "                                          arrow_scale=.3,\n",
        "                                          name=key,\n",
        "                                          line_width=1.5)\n",
        "\n",
        "            # Add the quiver plot to the main plot\n",
        "            fig.add_traces(data=quiver_fig.data)\n",
        "\n",
        "        # Update the layout properties of the plot\n",
        "        fig.update_layout(\n",
        "            autosize=True,\n",
        "            width=1200,\n",
        "            height=750,\n",
        "            margin=dict(l=10, r=10, b=10, t=20))\n",
        "\n",
        "        # Display the plot\n",
        "        fig.show()\n",
        "        \n",
        "    def plot_UMAP(self):\n",
        "        # Perform UMAP dimensionality reduction on the data\n",
        "        trans = umap.UMAP(random_state=42).fit(self.X)\n",
        "    \n",
        "        # Transform the original data points and store them in a new dictionary\n",
        "        for key, value in self.dic_categorias.items():\n",
        "            value_UMAP = trans.transform(value)\n",
        "            dic_categorias_aux = {key: value_UMAP}\n",
        "            self.dic_categorias_UMAP.update(dic_categorias_aux)\n",
        "    \n",
        "        # Transform the minimum distance points and store them in a new dictionary\n",
        "        for key, value in self.dic_min_dst.items():\n",
        "            value_UMAP = trans.transform(value)\n",
        "            dic_min_dst_aux = {key: value_UMAP}\n",
        "            self.dic_min_dst_UMAP.update(dic_min_dst_aux)\n",
        "    \n",
        "        # Set initial state for the loop\n",
        "        door = True\n",
        "        next_color = 0\n",
        "    \n",
        "        # Loop through the categories in the UMAP transformed dictionary\n",
        "        for key, value in self.dic_categorias_UMAP.items():\n",
        "            # If it's the first iteration, create a new plot with the first category's data\n",
        "            if door:\n",
        "                fig = go.Figure(data=[go.Scatter(x=value[:, 0], y=value[:, 1],\n",
        "                                                 mode='markers',\n",
        "                                                 name=key,\n",
        "                                                 marker=dict(\n",
        "                                                     size=6,\n",
        "                                                     # color=self.color_list[next_color],  # Set color to an array/list of desired values\n",
        "                                                     colorscale='picnic',  # Choose a colorscale\n",
        "                                                     opacity=0.7)\n",
        "                                                 )])\n",
        "                # Change the door variable to False, indicating that the first iteration is done\n",
        "                door = False\n",
        "                # Increment the color index\n",
        "                next_color += 1\n",
        "            else:\n",
        "                # For the rest of the categories, add them to the existing plot\n",
        "                fig.add_trace(go.Scatter(x=value[:, 0], y=value[:, 1],\n",
        "                                         mode='markers',\n",
        "                                         name=key,\n",
        "                                         marker=dict(\n",
        "                                             size=6,\n",
        "                                             # color=self.color_list[next_color],  # Set color to an array/list of desired values\n",
        "                                             colorscale='picnic',  # Choose a colorscale\n",
        "                                             opacity=0.7)\n",
        "                                         ))\n",
        "                # Increment the color index\n",
        "                next_color += 1\n",
        "    \n",
        "        # Loop through the minimum distance points dictionary and add them to the plot\n",
        "        for key, value_dst in self.dic_min_dst_UMAP.items():\n",
        "            fig.add_trace(go.Scatter(x=value_dst[:, 0], y=value_dst[:, 1],\n",
        "                                     mode='markers',\n",
        "                                     name=key,\n",
        "                                     marker=dict(\n",
        "                                         symbol=220,\n",
        "                                         size=14,\n",
        "                                         color=np.random.randint(100),  # Set color to an array/list of desired values\n",
        "                                         # colorscale='Viridis',  # Choose a colorscale\n",
        "                                         opacity=1)\n",
        "                                     ))\n",
        "    \n",
        "        # Update the layout properties of the plot\n",
        "        fig.update_layout(\n",
        "            autosize=True,\n",
        "            width=800,\n",
        "            height=600,\n",
        "            margin=dict(l=10, r=10, b=10, t=20))\n",
        "    \n",
        "        # Display the plot\n",
        "        fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7URyRwAV2UDG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}